{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>Sistem Temu Kembali Informasi</strong><br />\n",
    "</center>\n",
    "\n",
    "<strong>Outline pertemuan:</strong><br />\n",
    "<li> TFIDF</li>\n",
    "<li> BM-25</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58028582 0.         0.         0.58028582]\n",
      "Is this the first document?\n",
      "This is the first document.\n",
      "And this is the third one.\n",
      "This document is the second document.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "query='first time'\n",
    "query_vec= vectorizer.transform([query])\n",
    "results=cosine_similarity(X, query_vec).reshape((-1))\n",
    "print (results)\n",
    "for i in results.argsort()[::-1]:\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25215917 0.47406737 0.28462634 0.        ]\n",
      "ayah mencuci mobil digarasi dan menggoda ibu ibu\n",
      "ibu membeli sayur dan buah buahan\n",
      "Budi bermain bola ketika ibu ke pasar\n",
      "kakak sedang tidur di kamar\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "corpus = [\n",
    "    'Budi bermain bola ketika ibu ke pasar',\n",
    "    'ayah mencuci mobil digarasi dan menggoda ibu ibu',\n",
    "    'ibu membeli sayur dan buah buahan',\n",
    "    'kakak sedang tidur di kamar',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "query='ibu'\n",
    "query_vec= vectorizer.transform([query])\n",
    "results=cosine_similarity(X, query_vec).reshape((-1))\n",
    "print (results)\n",
    "for i in results.argsort()[::-1]:\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>BM-25 Rank Algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', 'there', 'good', 'man!'], ['It', 'is', 'quite', 'windy', 'in', 'London'], ['How', 'is', 'the', 'weather', 'today?']]\n",
      "['good', 'man']\n",
      "['Hello there good man!']\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"How is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "print (tokenized_corpus)\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "query = \"good man\"\n",
    "tokenized_query = query.split(\" \")\n",
    "print (tokenized_query)\n",
    "\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "bm25.get_top_n(tokenized_query, corpus, n=1)\n",
    "print (bm25.get_top_n(tokenized_query, corpus, n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Budi', 'bermain', 'bola', 'ketika', 'ibu', 'ke', 'pasar'], ['ayah', 'mencuci', 'mobil', 'digarasi', 'dan', 'menggoda', 'ibu', 'ibu'], ['ibu', 'membeli', 'sayur', 'dan', 'buah', 'buahan'], ['kakak', 'sedang', 'tidur', 'di', 'kamar']]\n",
      "['ibu']\n",
      "['ayah mencuci mobil digarasi dan menggoda ibu ibu']\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    'Budi bermain bola ketika ibu ke pasar',\n",
    "    'ayah mencuci mobil digarasi dan menggoda ibu ibu',\n",
    "    'ibu membeli sayur dan buah buahan',\n",
    "    'kakak sedang tidur di kamar',\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "print (tokenized_corpus)\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "query = \"ibu\"\n",
    "tokenized_query = query.split(\" \")\n",
    "print (tokenized_query)\n",
    "\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)\n",
    "bm25.get_top_n(tokenized_query, corpus, n=1)\n",
    "print (bm25.get_top_n(tokenized_query, corpus, n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\M-SI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mauris accumsan arcu ut turpis laoreet pretium 0.5366721\n",
      "2 Mauris at sapien consectetur tellus feugiat convallis 0.5067044\n",
      "3 Mauris lacinia magna a purus dignissim, ac bibendum metus gravida 0.391652\n",
      "4 Maecenas et mauris volutpat, scelerisque ante quis, iaculis metus 0.36695278\n",
      "5 Vivamus et felis ut ipsum aliquam fringilla 0.28199077\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# New corpus\n",
    "corpus = [\n",
    "    \"Mauris lacinia magna a purus dignissim, ac bibendum metus gravida\",\n",
    "    \"Mauris at sapien consectetur tellus feugiat convallis\",\n",
    "    \"Mauris accumsan arcu ut turpis laoreet pretium\",\n",
    "    \"Maecenas et mauris volutpat, scelerisque ante quis, iaculis metus\",\n",
    "    \"Vivamus et felis ut ipsum aliquam fringilla\",\n",
    "]\n",
    "\n",
    "# Tokenize the sentences to get a list of words\n",
    "words = [word for sentence in corpus for word in nltk.word_tokenize(sentence)]\n",
    "\n",
    "# Choose a random word from the corpus as the query\n",
    "query = \"mauris\"\n",
    "\n",
    "# Load pre-trained sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Encode corpus and query\n",
    "document_embeddings = model.encode(corpus)\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Compute cosine similarity between query and corpus\n",
    "cosine_similarities = cosine_similarity(query_embedding, document_embeddings)\n",
    "\n",
    "# Get index of the most relevant document\n",
    "most_relevant_doc_index = cosine_similarities.argmax()\n",
    "\n",
    "# print all documents along with their cosine similarity score and sort them\n",
    "num = 1\n",
    "for i in cosine_similarities.argsort()[0][::-1]:\n",
    "    print(num, corpus[i], cosine_similarities[0][i])\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Indexing (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Quisque sit amet tortor hendrerit, commodo odio ut, posuere turpis. 0.9926931\n",
      "2 Sed ullamcorper orci in odio suscipit, ac ultricies tortor iaculis. 0.9926931\n",
      "3 Praesent vitae odio laoreet, aliquet diam in, hendrerit sapien. 0.05814138\n",
      "4 Suspendisse mollis nisi vel enim maximus mollis. 0.0\n",
      "5 Cras vitae velit eu massa interdum varius. -0.2601207\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "corpus = [\n",
    "    \"Sed ullamcorper orci in odio suscipit, ac ultricies tortor iaculis.\",\n",
    "    \"Cras vitae velit eu massa interdum varius.\",\n",
    "    \"Suspendisse mollis nisi vel enim maximus mollis.\",\n",
    "    \"Quisque sit amet tortor hendrerit, commodo odio ut, posuere turpis.\",\n",
    "    \"Praesent vitae odio laoreet, aliquet diam in, hendrerit sapien.\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(tokenized_corpus)\n",
    "\n",
    "# Convert the corpus into Bag of Words using dictionary\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]\n",
    "\n",
    "# Create an LSI transformation of the corpus\n",
    "lsi = models.LsiModel(bow_corpus, id2word=dictionary, num_topics=2)\n",
    "\n",
    "# Create a similarity index\n",
    "index = similarities.MatrixSimilarity(lsi[bow_corpus])\n",
    "\n",
    "# Query transformation\n",
    "query = 'in'\n",
    "query_bow = dictionary.doc2bow(query.split())\n",
    "query_lsi = lsi[query_bow]\n",
    "\n",
    "# Perform a similarity query against the corpus\n",
    "sims = index[query_lsi]\n",
    "\n",
    "# print all documents along with their similarity score and sort them\n",
    "num = 1\n",
    "for i in sims.argsort()[::-1]:\n",
    "    print(num, corpus[i], sims[i])\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Nulla at odio at odio pharetra tempus et ut lacus. 0.5345224838248488\n",
      "2 Aliquam interdum odio suscipit, vulputate nulla sit amet, ultricies purus. 0.31622776601683794\n",
      "3 Sed tempus sapien sit amet est luctus, ac tristique metus ultrices. 0.0\n",
      "4 Integer hendrerit urna a orci vulputate, sed pharetra enim vehicula. 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    'Aliquam interdum odio suscipit, vulputate nulla sit amet, ultricies purus.',\n",
    "    'Nulla at odio at odio pharetra tempus et ut lacus.',\n",
    "    'Integer hendrerit urna a orci vulputate, sed pharetra enim vehicula.',\n",
    "    'Sed tempus sapien sit amet est luctus, ac tristique metus ultrices.',\n",
    "]\n",
    "\n",
    "# Query\n",
    "query = \"odio\"\n",
    "\n",
    "# Initialize CountVectorizer with binary=False\n",
    "vectorizer = CountVectorizer(binary=False)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Transform the query\n",
    "query_vec = vectorizer.transform([query])\n",
    "\n",
    "# Compute cosine similarity between query and corpus\n",
    "cosine_similarities = cosine_similarity(query_vec, X).flatten()\n",
    "\n",
    "# print all documents along with their cosine similarity score and sort them\n",
    "num = 1\n",
    "for i in cosine_similarities.argsort()[::-1]:\n",
    "    print(num, corpus[i], cosine_similarities[i])\n",
    "    num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1 0]\n",
      "Sed interdum nisi ultrices, ornare tellus nec, laoreet tortor.\n",
      "Sed gravida erat a facilisis ornare\n",
      "Sed pharetra urna sed ante ornare suscipit\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Sed interdum nisi ultrices, ornare tellus nec, laoreet tortor.\",\n",
    "    \"Donec maximus nibh at mi suscipit, tempor finibus purus rutrum\",\n",
    "    \"Sed gravida erat a facilisis ornare\",\n",
    "    \"Sed pharetra urna sed ante ornare suscipit\",\n",
    "    \"Duis dignissim urna in scelerisque gravida\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer with binary=True\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the result to an array\n",
    "binary_term_document_matrix = X.toarray()\n",
    "\n",
    "# Now you can use this binary_term_document_matrix for Boolean Retrieval\n",
    "# For example, to find documents that contain both 'document' and 'first', you can do:\n",
    "document_index = vectorizer.vocabulary_.get('sed')\n",
    "first_index = vectorizer.vocabulary_.get('ornare')\n",
    "\n",
    "# Use logical AND to find documents that contain 'document' and 'first'\n",
    "result = binary_term_document_matrix[:, document_index] & binary_term_document_matrix[:, first_index]\n",
    "\n",
    "# Print the result\n",
    "print(result)\n",
    "\n",
    "# print the document that contain both 'document' and 'first'\n",
    "for i, doc in enumerate(corpus):\n",
    "    if result[i]:\n",
    "        print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Duis dignissim urna in scelerisque gravida 0.27448673838643983\n",
      "2 Phasellus vel urna aliquet, ultricies libero eu, tincidunt diam 0.2201328763012063\n",
      "3 Integer hendrerit urna a orci vulputate, sed pharetra enim vehicula 0.2201328763012063\n",
      "4 Donec maximus nibh at mi suscipit, tempor finibus purus rutrum 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "corpus = [\n",
    "    'Integer hendrerit urna a orci vulputate, sed pharetra enim vehicula',\n",
    "    'Duis dignissim urna in scelerisque gravida',\n",
    "    'Phasellus vel urna aliquet, ultricies libero eu, tincidunt diam',\n",
    "    'Donec maximus nibh at mi suscipit, tempor finibus purus rutrum'\n",
    "]\n",
    "\n",
    "query = 'urna'\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True, norm=None)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_corpus = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Transform the query\n",
    "X_query = vectorizer.transform([query])\n",
    "\n",
    "# Compute the cosine similarity\n",
    "cosine_similarities = cosine_similarity(X_query, X_corpus)\n",
    "\n",
    "# Print the cosine similarities sort them\n",
    "num = 1\n",
    "for i in cosine_similarities.argsort()[0][::-1]:\n",
    "    print(num, corpus[i], cosine_similarities[0][i])\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: corpus text source was from lorem ipsum ^_^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
